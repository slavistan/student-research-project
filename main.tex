\documentclass{article}

% Packages
\usepackage[utf8]{inputenc} % Required for German umlauts
\usepackage{amsmath}
% SVG
\usepackage{svg}
% Bib
\usepackage[backend=biber]{biblatex}
\addbibresource{bib/bibliography.bib}
% Geometry
\usepackage{geometry}
\geometry{
  paper=a4paper, % Change to letterpaper for US letter
  inner=2.5cm, % Inner margin
  outer=3.8cm, % Outer margin
  bindingoffset=.5cm, % Binding offset
  top=1.5cm, % Top margin
  bottom=1.5cm, % Bottom margin
}
\usepackage[onehalfspacing]{setspace}
\usepackage{parskip}
%

%
% Tikz Setup
%
\usepackage{tikz}
\newcommand{\tikzmark}[2]{
    \tikz[overlay,remember picture,baseline]
    \node[anchor=base] (#1) {$#2$};
}
\usetikzlibrary{positioning}
%
% Font
%
\usepackage{fontspec}
%\defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
\setmainfont{Palatino Linotype}
%\setmonofont{Lucida Sans Typewriter}
%%
%% Custom commands
%%
\input{commands.tex}


\author{Stanislaw HÃ¼ll}
\title{Student Research Project}
\date{2018-11-11}
\begin{document}

\maketitle

\section{Introduction}

  \subsection{Compressed Sparse Row Format}
    It is a widely used storage format for sparse matrices which does not make assumptions about the matrix's shape
    or its distribution of non-zero elements in contrast to other popular sparse matrix storage formats such as the
    the diagonal storage format or ELLPACK. It optimizes the storage requirements of general sparse matrices with
    respect to the naive coordinate format (COO) in that each non-zero's row index is no longer explicitly stored.

    The CSR format consists of three dense arrays:

    The values array stores the numerical value for each non-zero entry in the matrix while their column index is stored
    in the associated column-index array. A third array, the row-pointer array, encodes the beginning of each row's
    section within the values and column-index arrays, i.e. it stores the offset of each row's first non-zero element
    into the two previous arrays. Thus the CSR format optimizes the storage requirements of general sparse matrices with
    respect to the naive coordinate format (COO) in that each-nonzero's row index is no longer explicitly stored,
    shrinking the size of the third array from one entry per non-zero element to a single entry per row.


    By convention, the non-zeros are stored row-wise in ordered fashion from left to right implying that each row's
    section within the column-index array is sorted in ascending fashion. Additionally, the row-pointer array contains
    an additional element denoting the total number of non-zero elements in the structure. Note that sometimes a
    different nomenclature is utilized in the existing literature, referring to the arrays as A (values), JA
    (column-indices) and IA (row-pointers), respectively \cite{sparskit}.

    The CSR format's salient feature is its direct access to a row's non-zero elements' values and column indices making
    it particularly well suited for matrix-vector-multiplication utilizing a conventional row-by-column computation
    scheme. Aside of this feature storing the non-zero elements row by row as opposed to column by column is, to a
    certain degree, arbitrary and thus exist numerical libraries and toolkits such as the Eigen C++ library
    \cite{eigen:website} or the Harwell-Boeing sparse matrix collection \cite{harwell-boeing} which utilize the CSR
    format's conjugate, the compressed sparse column format (CSC), as their default means of representing sparse
    matrices.

  \subsection{Structured Grid Matrices}

    Structured grid computations are ubiquitously used for physical simulations in scientific fields such as, for example, computational fluid dynamics, electrodynamics and astrophysics. Generally, a system of partial differential equations is solved by discretization and linearization of the problem which involves generating a grid corresponding to the physical domain in question. A structured grid refers to a mesh whose nodes are logically rectangular, i.e. they are uniquely identified by a tuple of coordinates and v.v., such as $(i, j, k)$ for three dimensions (Figure \ref{fig:structured_grid_example}).

    \begin{figure}[ht]
        \centering
        \vspace{0.5cm}
        \begin{minipage}{0.4\textwidth}
          \centering
          \includegraphics[width=0.9\textwidth]{fig/structured_grid_example.png} % second figure itself
          \caption{\textbf{Curvi-linear grid over a physically complex domain.} The grid nodes are logically rectangular despite the non-rectangular physical domain and thus comprise a structured grid. Source: \cite{daad:website}}
          \label{fig:structured_grid_example}
        \end{minipage}\hfill
        \begin{minipage}{0.5\textwidth}
          \centering
          \includegraphics[width=0.9\textwidth]{fig/refined_structured_grid.png} % second figure itself
          \caption{\textbf{Heterogeneous mesh with structured regions.} Outer side (green) and interior volume are fully structured and are connected via an unstructured mesh region. Given proper indexing of grid nodes the resulting sparse matrix will exhibit regularity in the sections pertaining to the structured mesh region. Source: \cite{cubit-mesh-refinement:website}}
          \label{fig:refined_structured_grid}
        \end{minipage}
    \end{figure}

    In contrast to unstructured grids the regularity inherent to structured grids allows for very efficient numerical treatment, such that even in cases where sufficiently complex geometries prohibit the decomposition of the target domain into a single overarching structured grid the domain is often tesselated into an unstructured configuration, with the tiles being filled by independent structured grids \cite{Badcock2000}(Figure \ref{fig:refined_structured_grid}).

    \begin{figure}
        \centering
        \begin{minipage}{0.45\textwidth}
          \centering
          \input{fig/laplacian_example.tikz}
        \end{minipage}\hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            \includegraphics[width=0.9\textwidth]{fig/laplacian_example.png} % second figure itself
        \end{minipage}
        \caption{\textbf{Structured grid and corresponding sparse adjacency matrix for a simple Laplacian} $\nabla^2 u(\vec{r}) = 0$. The Laplace-operator is approximated by the conventional 3-axial symmetric discretization scheme, equivalent to a symmetric 7-point stencil operation.}
        \label{fig:laplacian_example}
    \end{figure}

    The solution procedure yields a sparse linear system whose solution is approximated by iterative methods involving repeated sparse matrix-vector-multiplications $Ax = b$ where the matrix $A$ encodes the adjacency structure of the grid. For a structured grid the resulting matrix's distribution of non-zeros has a very distinct pattern. Generally speaking, such matrices consist of multiple diagonals of non-zero values at fixed intervals while all other elements are zero. The diagonals are almost fully dense with exceptions arising at positions corresponding to nodes at the grid's boundaries, where the regular adjacency pattern of the grid is disturbed by missing nodes. A grid and its corresponding sparse matrix's structure are depicted in Figure \ref{fig:laplacian_example} for a simple Laplacian problem.

    The matrix's non-zero values depend on the underlying physical problem's parameters, the type of PDE and the differential operators' discretization schemes. In general, the non-zeros' numeric values are independent from their location within the matrix but there exist classes of problems where the equivalence of the structure of a set of matrix rows also implies that those rows' non-zeros share the same numeric values. Furthermore, solving problems on higher-dimensional entities such as vectors or matrices yields structured matrices whose 'elements' are themselves square matrices \cite{Godwin2013}.

    The sparse matrices relevant to this work areany sparse matrices which contain one or more sets of rows whose non-zero's column indices are identical save for a fixed offset. Structured grid matrices arising from the procedure mentioned above have one such set to which the overwhelming majority of rows belong corresponding to all inner grid nodes (such as in the example in Figure \ref{fig:laplacian_example}). However, all of the ideas presented hereinafter may be applied to any sparse matrix whose structure exhibits any degree of repetiveness.

\section{Three-fold compressed sparse row}

// TODO: Section with nomenclature
  - structure, pattern, ...

  Evidently, representing a structured grid's adjacency matrix using the regular CSR format is highly suboptimal, as it has no means of capturing the apparent repetetiveness of the structure. While at first glance the diagonal format might seem an appealing choice for the types of matrices introduced in the previous section, real-life problems produce matrices which are only locally structured, i.e. they contain multiple fully structured sections corresponding to the multiple structured grid regions of the overall heterogeneous domain mentioned above, which need not be aligned in a way to produce a diagonal structure at all. Thus a more flexible approach is taken adapting the CSR format.

  Additionally, utilizing a CSR-like format keeps open the possibility to eventually tackle adjacency matrices of
  unstructured grids.

  This section introduces the data layout and storage scheme of the three-fold compressed sparse row format which allows
  for the utilization of certain compression mechanisms explained thereafter. Finally, the algorithms required to
  perform common arithmetic operations are presented.

  \subsection{Data layout and storage scheme}


    It is cruicial to observe that, in the most general case, the index patterns' regularity is not shared by the non-zero elements' numerical values. While two or more rows may share the same pattern their corresponding values need not be similar to each other at all and, possibly, even vice versa. To prevent that a lack of common regularity impedes optimizing the storage requirements of one or the other it is hence necessary to decouple the representation of a row's column index positions from the representation of its numerical values. The C3SR format accounts for this circumstance and maintains separate data structures for the patterns and values.

    //TODO Erst colindices einfÃ¼hren, dann values und darauf hinweise, dass VS redundant ist, falls keine values
    compression erwÃ¼nscht ist.

    The values are represented by two arrays which utilize the CSR's basic idea of storing all relevant data in an array (V) and managing a separate start-index array (VS) pointing each row's section's first element.

      ((( TODO: Bild. Einfache matrix mit values )))

    The column indices are stored abiding by the same principle with the additional detail that each row's column indices are decomposed into the absolute index of the row's first non-zero element, which is referred to as the row's 'peg', and the relative offsets of the remaining non-zeros' column indices with respect to the peg index.  This introduces a degree of freedom whose usefulness will become evident in the context of compression. Thus the column indices are encoded into an array storing the patterns (J), another array storing the peg index (JP) and an index pointer array (JS) into J in the same way that VS relates to V.

      ((( TODO: Bild )))

    A sixth and final array (RS) is used to encode the number of non-zeros in each row of the matrix in order to know the size of the row's section within the pattern array and the values array. The RS array is an immediate copy of the CSR format's row-pointer array.  While the respective start index-pointer arrays store the section's first element's index they cannot encode the size of the section. Prior to compressing the size of a row can indeed be retrieved from the corresponding start index-pointer array in the same way the CSR format's row size may be retrieved from the row-pointer array by taking the difference between the row's entry and its successor, however, this requires that the row's sections are stored contiguously in the arrays and this property is lost during compression as will be explained in the next section.

    Summarizing the data organization of the C3SR format:
    \begin{itemize}
      \item Data representing the matrix's non-zero elements: V, VS
      \item Data representing non-zero elements' column indices: J, JP, JS
      \item Array containing information about the row sizes: RSS
    \end{itemize}

  \subsection{Compression mechanism}

    Using this storage scheme the column index information may be compressed by removing duplicate patterns from J and
    updating the start-index pointers JS of such duplicate rows to point the start of the corresponding unique section
    within J.

      ((( TODO: Bild )))

    The information about the non-zeros' values can be compressed in precicely the same way: Duplicate sections within V
    corresponding to different rows with identical values are removed and the corresponding start pointers are updated.

    In theory, the compression of patterns and values are orthogonal operations and if the overall goal were to minimize
    the storage requirement of a C3SR object in memory the best advice would be to exploit duplication in the matrix as
    much as possible. However, efficient arithmetic requires considerations going beyond maximum compression such as
    parallel performance and parallel scalability implying that the implementation must heed the principles of
    data locality and independence of tasks in the control flow of the algorithm.

    (((TODO: Partitioning during compression + stable- and non-stable values compression)))

    Note that this section avoids reasoning about low-level details for the sake of clarity. Tweaks such as the omission
    of each pattern's first value, which is always zero or mapping two partially matching rows' sections within J onto
    each other for a little gain in storage space optimization are considered implementation details are are not discussed
    here.

  \subsection{Matrix-Vector Multiplication}

    \subsubsection{Basic CSR Multiplication scheme}

      Algorithmically, the basic multiplication schemes of the C3SR format and the CSR format are very similar. Both access their respective non-zeros by an index pointer into an array and compute the argument vector's access index in the same manner. Differences arise only in the C3SR's additional offset $\JP[k]$ which is applied to each relative column index.

      C3sr:
      $$\sum \limits_{\alpha = 0}^{\RSS[k] - 1} \V[\VS[k] + \alpha] \cdot x[\J[\JS[k] + \alpha] + \JP[k]]$$

      CSR:
      $$\sum \limits_{\alpha = 0}^{\RP[k] - 1} \V[\RP[k] + \alpha] \cdot x[\CI[\RP[k] + \alpha]]$$

      However, the fact that the C3SR format utilizes additional data structures to store information leads to additional accesses to memory. Assuming the general case of a large matrix devoid of any regularity in its structue whose size exceeds the machine's cache the CSR format's arithmetic performance will be better proportional to the reduction of loads from memory with respect to the C3SR format as this is a memory bound computation involving few trivial arithmetic operations.

      On the flipside, structured matrices facilitate an optimized representation allowing a small segment of memory, corresponding to possibly only a few cache lines, to contain the matrix's complete structural information. This can drastically improve arithmetic performance of the C3SR format despite the more complex memory access scheme, as will be shown in the benchmark section.

    \subsubsection{SIMD Implementation}

      In addition to the performance gain utilizing the basic multiplication scheme, the data layout of a structured matrix in C3SR format allows for the matrix-vector multiplication to be implemented in such a way, as to utilize SIMD parallelism. The matrix's data is layed out in memory in such a way that the matrix-vector multiplication may be performed for multiple rows at a time using vectorization. Depending on the composition of the matrix three different, yet very similar, multiplication schemes can be devised.

      Suppose that a horizontal slice of rows $r, \ldots, r+k$ has diagonal structure with the diagonals starting at indices $s, t, \ldots, u$, such as is depicted in TODO. The matrix-vector multiplication for each of the slice's rows is composed of the same number of summands, one per diagonal. Each of the summands is a product of the diagonal's non-zero entry and the argument's corresponding element, starting out with $a_{r,s} \cdot x_s$ for the first diagonal's initial element and $a_{r,t} \cdot x_t$ for the second diagonal. For each subsequent row the matrix element's indices are incremented aswell as the argumnet vector's index, commencing with $a_{r+1, s_1} \cdot x_{s+1}$ for the first diagonal's second element and accordingly for each other diagonal.

      As each diagonal's summand accesses the argument vector's elements in consecutive fashion, for example $x_s, x_{s+1}, \ldots, x_{s + k}$ for the first diagonal, the multiplication can be vectorized. The other summand's constituents, the diagonal's matrix entries, are accessed at a fixed offset which is equal to the number of diagonals. This is due to the fact that the matrix slice's non-zero elements are stored row-by-row, where each row has the same number of elements. Thus $a_{r,s}$ is located as many elements before $a_{r+1, r+2}$ within \V as there are diagonals. 

      \begin{equation}
        \begin{pmatrix}
          \\
          \cdots & 0 & \tikzmark{diagAFirst}{a_{r,s}} &  0 & \cdots & \tikzmark{diagBFirst}{a_{r,t}} & 0 & \cdots & \cdots & \cdots \\
          \cdots & \cdots & 0 & a_{r+1,s+1} & 0 & \cdots & a_{r+1,t+1} & 0 & \cdots & \cdots \\
          \cdots & \cdots & \cdots & 0 & \tikzmark{diagALast}{a_{r+2,s+2}} & 0 & \cdots & \tikzmark{diagBLast}{a_{r+2,t+2}} & 0 & \cdots \\
          \\
        \end{pmatrix}
      \end{equation}

      \begin{tikzpicture}[overlay,remember picture]
           \draw[color=red,opacity=0.1,line width=1mm,line cap=round] (diagAFirst.center) -- (diagALast.center);
           \draw[color=green,opacity=0.1,line width=1mm,line cap=round] (diagBFirst.center) -- (diagBLast.center);
      \end{tikzpicture}

      \begin{equation}
        \begin{matrix}
          \begin{bmatrix}
            a_{r,s}     \\
            a_{r+1,s+1} \\
               \vdots   \\
            a_{r+k,s+k} \\
          \end{bmatrix} & \cdot & \begin{bmatrix}
                                    x_s      \\
                                    x_{s+1}  \\
                                      \vdots \\
                                    x_{s+k}  \\
                                  \end{bmatrix} & + & \begin{bmatrix}
                                                      a_{r,t}     \\
                                                      a_{r+1,t+1} \\
                                                        \vdots    \\
                                                      a_{r+k,t+k} \\
                                                      \end{bmatrix} & \cdot & \begin{bmatrix}
                                                                                x_t \\
                                                                                x_{t+1} \\
                                                                                \vdots \\
                                                                                x_{t+k}
                                                                              \end{bmatrix} & + & \cdots & = \begin{bmatrix}
                                                                                                                 y_{r} \\
                                                                                                                 y_{r+1} \\
                                                                                                                 \vdots \\
                                                                                                                 y_{r+k}
                                                                                                                \end{bmatrix}
        \end{matrix}
      \end{equation}

      In SIMD terms a stride-gather and a load is required for the diagonal's elements and the argument vector, respectively. The two vector registers are then multiplied and the result is then added onto the product pertaining to the next diagonal, as depicted in TODO.

      For the case of a matrix slice whose structure is diagonal and whose values within each diagonal are identical, e.g. $a_{r,s} = a_{r+1, s+1} = \ldots$ for the first diagonal etc., the multiplication scheme is simplified in that the argument vector's values contained in the vector register are simply scaled by the diagonal's value instead of being subjected by a vectorized multiplication.

      \begin{equation}
        \begin{matrix}
          a_{r,s} & \cdot & \begin{bmatrix}
                                    x_s      \\
                                    x_{s+1}  \\
                                      \vdots \\
                                    x_{s+k}  \\
          \end{bmatrix} & + & a_{r,t} & \cdot & \begin{bmatrix}
                                                x_t \\
                                                x_{t+1} \\
                                                \vdots \\
                                                x_{t+k}
                                                                              \end{bmatrix} & + & \cdots & =  \begin{bmatrix}
                                                                                                                 y_{r} \\
                                                                                                                 y_{r+1} \\
                                                                                                                 \vdots \\
                                                                                                                 y_{r+k}
                                                                                                                \end{bmatrix}
        \end{matrix}
      \end{equation}

      Another practically relevant case are matrix slices whose structure is uniform, i.e. each row's non-zero elements are situated at the same columns. The corresponding vectorized matrix-vector multiplication scheme is again similar to the general case of a diagonal structure. Here the argument vector's elements serve as scaling constants for the vector register containing the matrix's elements, which are again located at fixed offsets within \V  as depicted in TODO.

      \begin{equation}
        \begin{pmatrix}
          \\
          \cdots & 0 & a_{r,s} &  0 & \cdots & a_{r,t} & 0 & \cdots \\
          \cdots & 0 & a_{r+1,s} & 0 & \cdots & a_{r+1,t} & 0 & \cdots \\
          \cdots & 0 & a_{r+2,s} & 0 & \cdots & a_{r+2,t} & 0 & \cdots \\
          \\
        \end{pmatrix}
      \end{equation}

      \begin{equation}
        \begin{matrix}
          \begin{bmatrix}
            a_{r,s}     \\
            a_{r+1,s+1} \\
               \vdots   \\
            a_{r+k,s+k} \\
          \end{bmatrix} & \cdot & x_s & + & \begin{bmatrix}
                                              a_{r,t}     \\
                                              a_{r+1,t+1} \\
                                                \vdots    \\
                                              a_{r+k,t+k} \\
                                            \end{bmatrix} & \cdot & x_t & + \cdots & = \begin{bmatrix}
                                                                                       y_{r} \\
                                                                                       y_{r+1} \\
                                                                                       \vdots \\
                                                                                       y_{r+k} \\
                                                                                     \end{bmatrix}
        \end{matrix}
      \end{equation}

      All of the above mentioned arithmetic schemes can be extended to vector registers of arbitrary length and are thus theoretically able to scale with additional hardware cabalities.

\section{Performance Benchmarks}
  Measure performance in terms of (1) data compression ratio and (2) arithmetic performance (matvecmult)

  \subsection{Generation of structured grid adjacency matrices as test matrices}

    For the purpose of gauging the performance test matrices are created in CSR format which are then converted to C3SR
    format and compressed. These matrices resemble the structure of the structured grid matrices introduced above and
    are created by iterating through a 3D grid of fixed integral dimensions X, Y, Z and applying a given stencil
    which encodes the desired adjacency relationship. Nodes requested by the stencil but missing from the grid, i.e.
    nodes on the grid's outer borders are omitted, i.e. their corresponding entries in the adjacency matrix carry a zero
    which is equivalent to Dirichlet-type boundary conditions.

    The matrix's non-zero entries' numeric values are obtained from evaluating a sinusoidal function at the geometric
    center point $\vec{r} = (x, y, z)$ inbetween the two nodes in question whereby each Cartesian component $x, y, z$ is
    scaled by its coordinate's span $X, Y, Z$. An additional offset serves to prevent that entries in the matrix
    correponding to adjacent nodes according to the stencil incidentally evaluate to 0. The function utilized is
    $$W(x,y,z; n_x, n_y, n_z) = 2 + \sum \limits_{d \in \{x,y,z\}} \sin{\frac{d}{d_{\text{max}}} \cdot \pi \cdot n_d} $$
    where $n_x, n_y, n_z$ are periodicity parameters for each dimensions.

    Note that $n_d = 0 \Leftrightarrow \partial_d W(\vec{r}) \equiv 0$, introducing a periodicity in the corresponding
    dimension $d$. This feature is utilized to control the periodicity in the matrix's values.

    (((Example of matrix + Computation of nonzeros)))

    For the purpose of this work two different sets of parameters are utilized generating two different matrices: A smaller matrix generated from a $100 \times 100 \times 100$ grid with the periodicities $n_x = n_y = n_z = 0$ and a second, larger matrix based on the same grid but different periodicities $n_x = 1.1; n_y = 1.2; n_z = 1.3$. Both matrices are generated from a symmetric 7p-stencil.

    The matrices' stats are displayed:

    \begin{tabular}{ l c c }
             & Small & Large \\
      \hline                        \\
      V      & 135    & 6940000 \\
      VS     & 1000000 & 1000000 \\
      J      & 135    & 135 \\
      JS     & 1000000 & 1000000 \\
      JP     & 1000000 & 1000000 \\
      RSS    & 1000001 & 1000001 \\
    \end{tabular}

    Note that the ... Daten unterscheiden sich nur in V (--> selbe struktur)
    --> SIMD Multiplikation verwendet bei den Matrizen unterschiedliche implementierungen (s. SIMD arithmetik)
  \subsection{Data compression}

    This plot compares the number of array elements required in the C3SR format to store a fully structured matrix's column index information (green curve) against the baseline size of the equivalent CSR format matrix's column index array (red curve) depending on the partition size utilized for the structural compression. The C3SR format's recorded number of elements comprises the sum of the sizes of the J, JP and JS arrays, respectively. The RS array and the CSR format's row-pointer array are left out of the consideration as they are identical in size. The underlying $50 \times 50 \times 50$ grid contains $125000$ nodes to which the symmetric 7-pt stencil is applied. The numerical values of the matrix are of no concern for its structure.

    The C3SR format's diminishing storage requirements are caused solely by J (blue curve) as JS and
    JP are constant in size, each containing $125000$ elements, one per matrix row. The size of J
    exhibits an inverse linear relationship to the partition size, as can be inferred from the
    straight line's gradient in the double logarithmic plot. This is due to the fact that large
    grids mainly consist of inner nodes whose corresponding rows in the matrix all share the same pattern. Hence the overwhelming majority of partitions' column index information will condense to 7 elements within J (and one element per row for JS and JP) irrespective of the partition size with exceptions arising only around the few border nodes and their corresponding rows.

    The minimum size of J is determined by the matrix's unique patterns which correspond to 8 corner
    nodes a 4 adjacent nodes, 12 edge nodes a 5 adjacency nodes, 6 face nodes a 6 adjaceny nodes and
    the inner nodes with the full 7 adjacency nodes totalling $135$ entries. (TODO: Tikz 3d-draw
    representative of all nodes creating unique patterns).

    TODO: AusreiÃer erklÃ¤ren.
    \begin{figure}[ht]
      \centering
      \includegraphics[width=0.8\textwidth]{fig/structural-compression}
      \caption*{TODO}
    \end{figure}

    //TODO: Structural compression mit interleaved matrices

  \subsection{Arithmetic Performance of Matrix-Vector Multiplication}

    As this is a memory bound operation it is to be expected that the efficacy of simd parallism increases as the object size decreases.

    \begin{figure}[!ht]
      \centering
      \includegraphics[width=0.9\textwidth]{assets/arithmetic_performance}
      \caption{Arithmetic Performance}
    \end{figure}

\section{Summary}
  Advantages: General purpose, good performance, parallel scalability
  Disadv: Static structure (cannot add/remove elements)

OVERALLTODO:
  TODO: ... will be referred to as 'structured matrix' ==== adjacency matrix of structured grids assuming xyz-indexing

  "No need to fully compress the matrix" --> Diminishing returns; parallelizability

  TODO:
    - AbschÃ¤tzung Performance-Gain vs. Speicherzugriffe/MatrixgrÃ¶Ãe
    - KNL Cores haben simple Hardware --> Effekt von korrektem Code ist grÃ¶Ãer
    - KNL: Falls Chunks zu groÃ werden fÃ¤llt Working Set aus Cache
    - Wozu Partitionierung --> Threads kÃ¶nnen parallel arbeiten
    - Throughput-AbschÃ¤tzung (Bewegte Datenmenge aus DRAM / Dauer); A,x,y (jedes Element wird ~1 mal ausgelesen)

\printbibliography
\end{document}
